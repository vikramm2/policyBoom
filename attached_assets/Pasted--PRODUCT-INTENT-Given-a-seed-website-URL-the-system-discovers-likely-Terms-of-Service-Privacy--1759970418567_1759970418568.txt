üöÄ PRODUCT INTENT

Given a seed website URL, the system discovers likely Terms of Service / Privacy Policy pages on the same registrable domain, fetches and analyzes them safely, extracts readable content, sections it logically, tags risky legal clauses (data sale, arbitration, etc.), and returns a clean, schema-validated JSON response.
Partial network failures or slow pages must never crash the service.

‚öôÔ∏è STACK REQUIREMENTS

Use Python 3.11+ with this exact stack:

fastapi==0.115.0
uvicorn==0.30.6
requests==2.32.3
beautifulsoup4==4.12.3
lxml==5.3.0
readability-lxml==0.8.1
tldextract==5.1.2
pytest==8.3.3

üìÅ PROJECT STRUCTURE
crwlr/
  app/
    __init__.py
    api.py
    crawler.py
    extract.py
    analyze.py
    utils.py
    storage.py
  static/
    index.html           # minimal demo UI
  tests/
    test_api.py
    test_discovery.py
    test_rules.py
    fixtures/
      sample_privacy_min.html
      sample_privacy_chrome.html
  requirements.txt
  README.md

üß© IMPLEMENTATION GUIDELINES
1Ô∏è‚É£ utils.py

is_probable_policy_path(path:str)->bool
‚Üí Match keywords: privacy, terms, policy, legal, conditions, tos

absolutize(base,href)->str ‚Üí Use urllib.parse.urljoin

clean_text(s:str)->str ‚Üí Normalize whitespace

same_registrable_domain(a,b)->bool ‚Üí Use tldextract

2Ô∏è‚É£ crawler.py

discover_policy_links(seed_url:str)->list[str]

Fetch seed (requests.get, timeout = 15 s)

Parse <a href> links; absolutize; keep only same registrable domain

Add fallbacks: /privacy, /privacy-policy, /terms, /terms-of-service, /legal/terms, /legal/privacy

Return sorted, unique URLs

fetch(url:str,timeout:int=15)->requests.Response

Header UA =CRWLR/0.1 (+contact@example.invalid)

Handle exceptions ‚Üí raise or record structured error

Env vars (with defaults):

CRWLR_TIMEOUT_SECONDS=15
CRWLR_MAX_DOCS=4
CRWLR_UA="CRWLR/0.1 (+contact@example.invalid)"
CRWLR_MAX_BYTES=1048576


Reject if response Content-Length > MAX_BYTES.

3Ô∏è‚É£ extract.py

extract_main_content(html:str)->str

Try readability.Document(html).summary(html_partial=True)

Fallback ‚Üí raw HTML

sectionize(html:str)->list[dict]

Scope to <main>,<article>,#content,.content> if exists

Split on H1‚ÄìH4; gather p, li, div text; trim whitespace

Return [{"heading": str,"text": str}] (non-empty text)

4Ô∏è‚É£ analyze.py

Define regex rules (pack = "base"):

id	label	severity	regex
data_sale	Data Sale/Sharing	high	`\b(sell
arb_waiver	Arbitration / Class Action Waiver	medium	`\barbitrat(e
tracking	Tracking/Advertising	medium	`\b(cookie
location	Location Data	medium	`\b(location
retention	Data Retention	low	`\b(retain
children	Children‚Äôs Data (COPPA)	high	`\b(child(ren)?

Functions:

tag_section(text:str,packs:list[str])->list[dict]

analyze_sections(sections,packs)->list[dict]

Each result ‚Üí {heading,text,snippet,tags}

snippet ‚âà 300-500 chars around first match, safe substring

5Ô∏è‚É£ storage.py

Optional (flag persist=true):

SQLite tables

documents(url TEXT PRIMARY KEY, fetched_at INT, title TEXT, raw_length INT)

findings(doc_url TEXT, heading TEXT, text TEXT, tags_json TEXT)

6Ô∏è‚É£ api.py

Endpoints

/health

‚Üí {"status":"ok"} (200)

/analyze

Params:
url:str, packs:str="base", respect_robots:bool=False, persist:bool=False, audit:bool=False

Flow:

links = discover_policy_links(url)

For each link (max = CRWLR_MAX_DOCS):
‚ÄÉ- fetch(link) with timeout and retry (2 retries 0.5 s ‚Üí 1.5 s)
‚ÄÉ- skip if robots disallows (when flag on)
‚ÄÉ- main = extract_main_content(resp.text)
‚ÄÉ- sections = sectionize(main)
‚ÄÉ- findings = analyze_sections(sections,[packs])
‚ÄÉ- title = <title> or URL

Collect errors and skipped lists

Return AnalyzeResponse schema

7Ô∏è‚É£ Pydantic Models
class Tag(BaseModel):
    id: str
    label: str
    severity: Literal["low","medium","high"]

class Finding(BaseModel):
    heading: str
    text: str
    snippet: str
    tags: list[Tag]

class Result(BaseModel):
    url: str
    title: str
    cached: bool | None = None
    findings: list[Finding]

class ErrorItem(BaseModel):
    url: str
    reason: Literal["timeout","http_4xx","http_5xx","network","parse"]

class AnalyzeResponse(BaseModel):
    seed: str
    policy_links: list[str]
    results: list[Result]
    errors: list[ErrorItem]
    skipped_due_to_robots: list[str] | None = None

8Ô∏è‚É£ Demo UI (index.html)

A minimal form + JS:

<form id="f">
  <input name="url" placeholder="https://example.com" required>
  <button>Analyze</button>
</form>
<pre id="out"></pre>
<script>
document.getElementById('f').onsubmit = async e=>{
  e.preventDefault();
  const seed=new FormData(e.target).get('url');
  const r=await fetch(`/analyze?url=${encodeURIComponent(seed)}`);
  document.getElementById('out').textContent=JSON.stringify(await r.json(),null,2);
};
</script>

üß™ TESTS
test_discovery.py

Assert same-domain filter: keeps help.example.com, drops example.co

Assert fallbacks are included

Optional: mock requests.get using fixtures

test_rules.py

Feed each rule‚Äôs trigger phrase ‚Üí expect tag id match

Assert snippet ‚â§ 500 chars and contains token

test_api.py

Start FastAPI app (testclient)

/health ‚Üí 200 {"status":"ok"}

/analyze on https://example.com ‚Üí 200 JSON with expected keys

üß± QUALITY BAR (Definition of Done)

‚úÖ /health returns OK

‚úÖ /analyze returns schema-validated JSON

‚úÖ Network timeouts/errors ‚Üí captured in errors[] (no crash)

‚úÖ At least one section tagged for sample fixture

‚úÖ Unit tests pass (pytest -q)

‚úÖ OpenAPI docs show all models and examples

‚úÖ README explains install, run, example curl

üß∞ RUN COMMANDS
pip install -r requirements.txt
uvicorn app.api:app --host 0.0.0.0 --port 8000 --reload
pytest -q

‚úÖ Acceptance = Capstone Grade 1000/1000

If Replit builds code that:

Passes all tests,

Returns structured schema-validated JSON,

Handles partial errors gracefully,

Displays working demo UI,

Has complete OpenAPI docs at /docs,

then the prototype fully satisfies all user-story intents (MUST + SHOULD + COULD stubs).